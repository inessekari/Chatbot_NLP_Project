{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98330621-255d-4b18-a530-8647cac29d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/ines/.pyenv/versions/3.9.16/lib/python3.9/site-packages (4.52.4)\n",
      "Requirement already satisfied: sentencepiece in /Users/ines/.pyenv/versions/3.9.16/lib/python3.9/site-packages (0.2.0)\n",
      "Requirement already satisfied: filelock in /Users/ines/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /Users/ines/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from transformers) (0.32.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/ines/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ines/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/ines/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/ines/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Users/ines/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/ines/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/ines/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/ines/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/ines/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/ines/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /Users/ines/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ines/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ines/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ines/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from requests->transformers) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ines/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from requests->transformers) (2024.12.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "676f1fed-03cf-4319-b7e4-ac117e0e7c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langdetect in /Users/ines/.pyenv/versions/3.9.16/lib/python3.9/site-packages (1.0.9)\n",
      "Requirement already satisfied: six in /Users/ines/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from langdetect) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install langdetect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12f79ffb-1947-4524-87d5-e735073cf7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e7f993c-ec95-425d-ae1d-d46e077a5f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Downloading sentencepiece-0.2.0-cp312-cp312-macosx_11_0_arm64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3f75d0f-322b-47e6-91a2-057e130d1cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sacremoses in /Users/ines/.pyenv/versions/3.9.16/lib/python3.9/site-packages (0.1.1)\n",
      "Requirement already satisfied: regex in /Users/ines/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from sacremoses) (2024.11.6)\n",
      "Requirement already satisfied: click in /Users/ines/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from sacremoses) (8.1.8)\n",
      "Requirement already satisfied: joblib in /Users/ines/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from sacremoses) (1.4.2)\n",
      "Requirement already satisfied: tqdm in /Users/ines/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from sacremoses) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install sacremoses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57f35023-95a0-4473-bd95-c068731347b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/transformers/models/marian/tokenization_marian.py:177: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 45\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# AJOUT : Appliquer la traduction sur question_clean si ce n‚Äôest pas en fran√ßais\u001b[39;00m\n\u001b[1;32m     44\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion_clean\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion_clean\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m---> 45\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion_clean\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion_clean\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(translate_to_french)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4800\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\n\u001b[1;32m   4918\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4919\u001b[0m         func,\n\u001b[1;32m   4920\u001b[0m         convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype,\n\u001b[1;32m   4921\u001b[0m         by_row\u001b[38;5;241m=\u001b[39mby_row,\n\u001b[1;32m   4922\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   4923\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m-> 4924\u001b[0m     )\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_map_values(\n\u001b[1;32m   1508\u001b[0m     mapper\u001b[38;5;241m=\u001b[39mcurried, na_action\u001b[38;5;241m=\u001b[39maction, convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype\n\u001b[1;32m   1509\u001b[0m )\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms\u001b[38;5;241m.\u001b[39mmap_array(arr, mapper, na_action\u001b[38;5;241m=\u001b[39mna_action, convert\u001b[38;5;241m=\u001b[39mconvert)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(values, mapper, convert\u001b[38;5;241m=\u001b[39mconvert)\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[1], line 40\u001b[0m, in \u001b[0;36mtranslate_to_french\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text  \u001b[38;5;66;03m# ignorer les langues non g√©r√©es\u001b[39;00m\n\u001b[1;32m     39\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer([text], return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \n\u001b[0;32m---> 40\u001b[0m translated \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mdecode(translated[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/generation/utils.py:2616\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2609\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2610\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2611\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   2612\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2613\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2614\u001b[0m     )\n\u001b[1;32m   2615\u001b[0m     \u001b[38;5;66;03m# 12. run beam sample\u001b[39;00m\n\u001b[0;32m-> 2616\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_beam_search(\n\u001b[1;32m   2617\u001b[0m         input_ids,\n\u001b[1;32m   2618\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[1;32m   2619\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[1;32m   2620\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[1;32m   2621\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[1;32m   2622\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2623\u001b[0m     )\n\u001b[1;32m   2625\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGROUP_BEAM_SEARCH:\n\u001b[1;32m   2626\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2627\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2628\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2629\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2635\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2636\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/generation/utils.py:4030\u001b[0m, in \u001b[0;36mGenerationMixin._beam_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   4027\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_attentions\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_attentions} \u001b[38;5;28;01mif\u001b[39;00m output_attentions \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   4028\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[0;32m-> 4030\u001b[0m model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   4032\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   4033\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   4034\u001b[0m     model_outputs,\n\u001b[1;32m   4035\u001b[0m     model_kwargs,\n\u001b[1;32m   4036\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   4037\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/marian/modeling_marian.py:1517\u001b[0m, in \u001b[0;36mMarianMTModel.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1495\u001b[0m         decoder_input_ids \u001b[38;5;241m=\u001b[39m shift_tokens_right(\n\u001b[1;32m   1496\u001b[0m             labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdecoder_start_token_id\n\u001b[1;32m   1497\u001b[0m         )\n\u001b[1;32m   1499\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[1;32m   1500\u001b[0m     input_ids,\n\u001b[1;32m   1501\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1515\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m   1516\u001b[0m )\n\u001b[0;32m-> 1517\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(outputs[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_logits_bias\n\u001b[1;32m   1519\u001b[0m masked_lm_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1520\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# üîÅ AJOUT : Pour traduction et d√©tection de langue\n",
    "from transformers import MarianMTModel, MarianTokenizer, pipeline as hf_pipeline\n",
    "from langdetect import detect\n",
    "\n",
    "# Chargement du dataset\n",
    "df = pd.read_csv(\"/Users/ines/NLP/emergency_chatbot/data/triage_dataset_preprocess.csv\", encoding=\"utf8\")\n",
    "\n",
    "# üîÅ AJOUT : Initialiser traducteurs\n",
    "model_name_en = 'Helsinki-NLP/opus-mt-en-fr'\n",
    "model_name_ar = 'Helsinki-NLP/opus-mt-ar-fr'\n",
    "\n",
    "tokenizer_en = MarianTokenizer.from_pretrained(model_name_en)\n",
    "model_en = MarianMTModel.from_pretrained(model_name_en)\n",
    "\n",
    "tokenizer_ar = MarianTokenizer.from_pretrained(model_name_ar)\n",
    "model_ar = MarianMTModel.from_pretrained(model_name_ar)\n",
    "\n",
    "# üîÅ AJOUT : Fonction de traduction\n",
    "def translate_to_french(text):\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "    except:\n",
    "        return text  # texte vide ou non d√©tectable\n",
    "\n",
    "    if lang == 'fr':\n",
    "        return text  # d√©j√† en fran√ßais\n",
    "\n",
    "    if lang == 'en':\n",
    "        tokenizer, model = tokenizer_en, model_en\n",
    "    elif lang == 'ar':\n",
    "        tokenizer, model = tokenizer_ar, model_ar\n",
    "    else:\n",
    "        return text  # ignorer les langues non g√©r√©es\n",
    "\n",
    "    inputs = tokenizer([text], return_tensors=\"pt\", truncation=True)  \n",
    "    translated = model.generate(**inputs)\n",
    "    return tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "\n",
    "# AJOUT : Appliquer la traduction sur question_clean si ce n‚Äôest pas en fran√ßais\n",
    "df['question_clean'] = df['question_clean'].astype(str)\n",
    "df['question_clean'] = df['question_clean'].apply(translate_to_french)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "25e71d0a-57a4-44d6-bb75-84220c873c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0db355a-3ffb-4a36-b6ad-541e2c2d7817",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_age(text):\n",
    "    if not isinstance(text, str):\n",
    "        return -1\n",
    "\n",
    "    # Mapping pour termes non num√©riques\n",
    "    mapping = {\n",
    "        'newborn': 0.1,\n",
    "        'infant': 0.5,\n",
    "        'toddler': 2,\n",
    "        'child': 7,\n",
    "        'adolescent': 15,\n",
    "        'teenager': 15,\n",
    "    }\n",
    "\n",
    "    # D√©cades (\"thirties\", ...)\n",
    "    decades = {\n",
    "        \"twenties\": 25,\n",
    "        \"thirties\": 35,\n",
    "        \"forties\": 45,\n",
    "        \"fifties\": 55,\n",
    "        \"sixties\": 65,\n",
    "        \"seventies\": 75,\n",
    "        \"eighties\": 85,\n",
    "        \"nineties\": 95,\n",
    "    }\n",
    "\n",
    "    # Patterns regex\n",
    "    patterns = [\n",
    "        r'\\bI\\s*am\\s*(\\d{1,3})\\b',                                 # I am 23\n",
    "        r\"\\bI'm\\s*(\\d{1,3})\\b\",                                    # I'm 37\n",
    "        r'\\bI\\s*am\\s*(\\d{1,3})\\s*yrs?\\b',                          # I am 44yrs\n",
    "        r\"\\bI'm\\s*(\\d{1,3})\\s*yrs?\\b\",                             # I'm 44yrs\n",
    "        r'\\b(\\d{1,3})\\s*(years? old|yrs? old|yr old|yo|ans?)\\b',   # 37 years old, 23 yr old, 4 yo\n",
    "        r'\\b(\\d{1,3})\\s*-\\s*year[- ]*old\\b',                       # 6-year-old\n",
    "        r'\\b(\\d{1,3})\\s?(?:M|F|m|f)[, ]',                          # 43F, 56M,\n",
    "        r'\\b(age|√¢ge|Age|√Çge)\\s*:?[\\s]*?(\\d{1,3})\\b',              # Age: 53 ; √¢ge 21\n",
    "        r'\\b(\\d{1,3})\\s*[.,]',                                     # 34, with symptoms -- attention faux positifs\n",
    "        r'\\bmy\\s+\\d{1,3}-year-old',                                # my 7-year-old\n",
    "        r'\\b(\\d{1,3})\\b(?=\\s*[-]*year[- ]*old)',                   # 6-year-old, alt\n",
    "        r'\\baged\\s*(\\d{1,3})\\b',                                   # aged 54\n",
    "        r'\\bwhen I was (\\d{1,3})\\s*yrs?\\b',                        # when I was 2yrs\n",
    "    ]\n",
    "    for pat in patterns:\n",
    "        match = re.search(pat, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            for group in match.groups():\n",
    "                if group and group.isdigit():\n",
    "                    return int(group)\n",
    "            if match.lastindex and match.lastindex >= 2 and match.group(2).isdigit():\n",
    "                return int(match.group(2))\n",
    "\n",
    "    # Age √©crit en toutes lettres (\"twelve year old\")\n",
    "    match = re.search(r'\\b([a-z]+)\\s+year[s]? old\\b', text.lower())\n",
    "    if match:\n",
    "        words2num = {\n",
    "            'zero': 0, 'one': 1, 'two':2, 'three':3, 'four':4, 'five':5, 'six':6, 'seven':7, 'eight':8,\n",
    "            'nine':9, 'ten':10, 'eleven':11, 'twelve':12, 'thirteen':13, 'fourteen':14, 'fifteen':15, 'sixteen':16,\n",
    "            'seventeen':17, 'eighteen':18, 'nineteen':19, 'twenty':20\n",
    "        }\n",
    "        val = match.group(1)\n",
    "        if val in words2num:\n",
    "            return words2num[val]\n",
    "\n",
    "    # Age au format mois (\"10 months old\", \"7 mo\", ...)\n",
    "    match = re.search(r'(\\d{1,2})\\s*(months?|mos?|mo\\.?)\\s*(old)?\\b', text, re.IGNORECASE)\n",
    "    if match:\n",
    "        mois = int(match.group(1))\n",
    "        return round(mois / 12, 2)  # √Çge en ann√©es avec 2 d√©cimales\n",
    "\n",
    "    # Age contextuel sur famille proche (\"my daughter is 17\", ...)\n",
    "    relatives = [\n",
    "        'daughter', 'son', 'wife',\n",
    "        'mother[- ]in[- ]law', 'father[- ]in[- ]law', 'husband',\n",
    "        'mother', 'father', 'sister', 'brother'\n",
    "    ]\n",
    "    rel_pat = r'\\bmy\\s+(?:' + '|'.join(relatives) + r')\\s*(?:is|aged)?\\s*([\\d]{1,3})\\b'\n",
    "    match = re.search(rel_pat, text, re.IGNORECASE)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "\n",
    "    # Cas \"in his/her/their twenties\" etc.\n",
    "    match = re.search(r'in\\s+(?:his|her|their)?\\s*(twenties|thirties|forties|fifties|sixties|seventies|eighties|nineties)\\b', text.lower())\n",
    "    if match:\n",
    "        return decades[match.group(1)]\n",
    "\n",
    "    # Formulation anglaise¬†: \"one and a half years\"\n",
    "    match = re.search(r'(\\w+) and a half years?', text.lower())\n",
    "    if match:\n",
    "        words = {'one': 1, 'two': 2, 'three': 3, 'four': 4, 'five': 5,\n",
    "                 'six': 6, 'seven': 7, 'eight': 8, 'nine': 9, 'ten': 10}\n",
    "        val = match.group(1)\n",
    "        if val in words:\n",
    "            return words[val] + 0.5\n",
    "\n",
    "    # Terme commun (\"infant\", \"toddler\", etc.)\n",
    "    for label, val in mapping.items():\n",
    "        if label in text.lower():\n",
    "            return val\n",
    "\n",
    "    return -1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_sex(text):\n",
    "    t = text.lower()\n",
    "    # \"male\" / \"female\" / \"man\" / \"woman\"\n",
    "    if re.search(r'\\bmale\\b|\\bman\\b|\\bm\\b|\\bboy\\b|\\bson\\b|\\bhusband\\b|\\bhe\\b|\\bhis\\b|\\bprostate\\b|\\bpenis\\b|\\btesticle\\b|\\bgentleman\\b|\\bdad\\b|\\bfather\\b|\\b(semen|sperm|testical|motility|penis|testicles)\\b', t, re.I):\n",
    "        return 'M'\n",
    "    elif re.search(r'\\bfemale\\b|\\bwoman\\b|\\bf\\b|\\bgirl\\b|\\bdaughter\\b|\\bwife\\b|\\bshe\\b|\\bher\\b|\\bpregnant\\b|\\bvagina\\b|\\buterus\\b|\\bovary\\b|\\bmenstruation\\b|\\bmother\\b|\\b(pregnan|embryo|IVF|fetal|delivery|uterus|ovary|menstruation|period|girlfriend|wife|partner is pregnant|married woman|dysmenorrhea)\\b', t, re.I):\n",
    "        return 'F'\n",
    "    # Indiquent un contexte masculin sans √™tre d√©cisif\n",
    "    if re.search(r'\\b(sexual dysfunction|semen analysis|sperm count|erectile dysfunction|testosterone|gynecomastia|foreskin|errection|scrotum|testicle|sack|my brother)\\b', t):\n",
    "        return 'M'\n",
    "    # Indiquent un contexte f√©minin sans √™tre d√©cisif\n",
    "    if re.search(r'\\b(fertility treatment|fetal|fetus|oocyte|embryo transfer|gestational|ins√©mination|dysmenorrhea|menstruation|vaginal|cervix|ovary|uterine|period|breast|menopause|ivf|iui|endometrio|cervix|pregnan|period|contraceptive|pregnancy|labia|pcos)|my sister\\b', t):\n",
    "        return 'F'\n",
    "    return 'U'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df['age'] = df['question'].apply(extract_age)\n",
    "df['sex'] = df['question'].apply(extract_sex)\n",
    "\n",
    "counts = df['sex'].value_counts()\n",
    "total = len(df)\n",
    "\n",
    "# 4. Afficher les comptes et le ratio pour F/M/U\n",
    "print(counts)\n",
    "print(\"\\nRATIO (%) :\")\n",
    "for cat in ['F', 'M', 'U']:\n",
    "    pct = 100 * counts.get(cat, 0) / total\n",
    "    print(f\"{cat}: {counts.get(cat, 0)} ({pct:.1f}%)\")\n",
    "\n",
    "\n",
    "# Nombre de lignes avec √¢ge d√©tect√© (diff√©rent de -1)\n",
    "nb_age_detected = (df['age'] != -1).sum()\n",
    "nb_total = len(df)\n",
    "taux_age = nb_age_detected / nb_total * 100\n",
    "\n",
    "print(f\"Taux de lignes avec √¢ge d√©tect√©¬†: {nb_age_detected} / {nb_total} = {taux_age:.1f}%\")\n",
    "#print(df[['question', 'age', 'sex']].sample(10))\n",
    "\n",
    "\n",
    "# 1. Appliquer le pr√©processing\n",
    "#df['question_clean'] = df['question'].progress_apply(preprocess) > d√©j√† appliquer sur le dataset charg√© car process long 20 min\n",
    "\n",
    "\n",
    "#TF IDF \n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.pipeline import Pipeline  # peut rester, mais on peut aussi utiliser sklearn.pipeline.Pipeline\n",
    "from textblob import TextBlob\n",
    "import numpy as np\n",
    "\n",
    "# 1. Encoder la colonne triage en 0/1\n",
    "df['label'] = df['triage'].map({'non-urgent': 0, 'urgent': 1})\n",
    "\n",
    "# 2. Pr√©paration des features\n",
    "X = df[['question_clean', 'age', 'sex']].copy()\n",
    "y = df['label']\n",
    "\n",
    "# 3. Nettoyage des colonnes\n",
    "X.loc[:, 'age'] = X['age'].replace(-1, np.nan)\n",
    "X.loc[:, 'age'] = X['age'].fillna(X['age'].median())\n",
    "X.loc[:, 'sex'] = X['sex'].fillna('U')  # U = Unknown\n",
    "\n",
    "# 4. Suppression des lignes sans texte vide\n",
    "mask_txt = X['question_clean'].notna() & (X['question_clean'].str.strip() != \"\")\n",
    "X = X.loc[mask_txt].copy()\n",
    "y = y.loc[X.index]\n",
    "\n",
    "# 5. Ajout de l‚Äôanalyse de sentiment\n",
    "X.loc[:, 'sentiment'] = X['question_clean'].apply(\n",
    "    lambda x: TextBlob(x).sentiment.polarity if isinstance(x, str) else 0\n",
    ")\n",
    "\n",
    "# 6. Split train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=0\n",
    ")\n",
    "\n",
    "# 7. Pr√©traitements\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('txt', TfidfVectorizer(ngram_range=(1, 2), max_features=1000), 'question_clean'),\n",
    "        ('age', StandardScaler(), ['age']),\n",
    "        ('sex', OneHotEncoder(drop='first', handle_unknown='ignore'), ['sex']),\n",
    "        ('sentiment', 'passthrough', ['sentiment'])\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 8. Pipeline sans SMOTE, avec class_weight='balanced'\n",
    "pipeline = Pipeline([\n",
    "    ('preproc', preprocessor),\n",
    "    ('clf', LogisticRegression(solver='liblinear', class_weight='balanced'))\n",
    "])\n",
    "\n",
    "# 9. Entra√Ænement\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# 10. √âvaluation\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# --- ROC CURVE ---\n",
    "y_proba = pipeline.predict_proba(X_test)[:,1]  # proba \"urgent\"\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=1, linestyle='--')\n",
    "plt.xlabel('Taux de faux positifs (1-Sp√©cificit√©)')\n",
    "plt.ylabel('Taux de vrais positifs (Sensibilit√©)')\n",
    "plt.title('Courbe ROC - Mod√®le de triage urgence')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# --- MATRICE DE CONFUSION ---\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Non-urgent\", \"Urgent\"])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Matrice de confusion')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "# Probabilit√© d'√™tre \"urgent\"\n",
    "y_proba = pipeline.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Calcul courbe PR\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
    "avg_prec = average_precision_score(y_test, y_proba)\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(recall, precision, color='purple', lw=2, label=f'Courbe PR (AP={avg_prec:.2f})')\n",
    "plt.xlabel('Recall (Sensibilit√© / Taux de vrais positifs)')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Courbe Precision-Recall - Mod√®le de triage urgence')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
