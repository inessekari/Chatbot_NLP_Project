{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98330621-255d-4b18-a530-8647cac29d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/ines/.pyenv/versions/3.9.16/lib/python3.9/site-packages (4.52.4)\n",
      "Requirement already satisfied: sentencepiece in /Users/ines/.pyenv/versions/3.9.16/lib/python3.9/site-packages (0.2.0)\n",
      "Requirement already satisfied: filelock in /Users/ines/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /Users/ines/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from transformers) (0.32.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/ines/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ines/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/ines/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/ines/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Users/ines/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/ines/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/ines/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/ines/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/ines/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/ines/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /Users/ines/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ines/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ines/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ines/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from requests->transformers) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ines/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from requests->transformers) (2024.12.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "676f1fed-03cf-4319-b7e4-ac117e0e7c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langdetect in /Users/ines/.pyenv/versions/3.9.16/lib/python3.9/site-packages (1.0.9)\n",
      "Requirement already satisfied: six in /Users/ines/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from langdetect) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install langdetect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12f79ffb-1947-4524-87d5-e735073cf7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e7f993c-ec95-425d-ae1d-d46e077a5f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Downloading sentencepiece-0.2.0-cp312-cp312-macosx_11_0_arm64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3f75d0f-322b-47e6-91a2-057e130d1cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sacremoses in /Users/ines/.pyenv/versions/3.9.16/lib/python3.9/site-packages (0.1.1)\n",
      "Requirement already satisfied: regex in /Users/ines/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from sacremoses) (2024.11.6)\n",
      "Requirement already satisfied: click in /Users/ines/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from sacremoses) (8.1.8)\n",
      "Requirement already satisfied: joblib in /Users/ines/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from sacremoses) (1.4.2)\n",
      "Requirement already satisfied: tqdm in /Users/ines/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from sacremoses) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install sacremoses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f35023-95a0-4473-bd95-c068731347b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/transformers/models/marian/tokenization_marian.py:177: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# üîÅ AJOUT : Pour traduction et d√©tection de langue\n",
    "from transformers import MarianMTModel, MarianTokenizer, pipeline as hf_pipeline\n",
    "from langdetect import detect\n",
    "\n",
    "# Chargement du dataset\n",
    "df = pd.read_csv(\"/Users/ines/NLP/emergency_chatbot/data/triage_dataset_preprocess.csv\", encoding=\"utf8\")\n",
    "\n",
    "# üîÅ AJOUT : Initialiser traducteurs\n",
    "model_name_en = 'Helsinki-NLP/opus-mt-en-fr'\n",
    "model_name_ar = 'Helsinki-NLP/opus-mt-ar-fr'\n",
    "\n",
    "tokenizer_en = MarianTokenizer.from_pretrained(model_name_en)\n",
    "model_en = MarianMTModel.from_pretrained(model_name_en)\n",
    "\n",
    "tokenizer_ar = MarianTokenizer.from_pretrained(model_name_ar)\n",
    "model_ar = MarianMTModel.from_pretrained(model_name_ar)\n",
    "\n",
    "# üîÅ AJOUT : Fonction de traduction\n",
    "def translate_to_french(text):\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "    except:\n",
    "        return text  # texte vide ou non d√©tectable\n",
    "\n",
    "    if lang == 'fr':\n",
    "        return text  # d√©j√† en fran√ßais\n",
    "\n",
    "    if lang == 'en':\n",
    "        tokenizer, model = tokenizer_en, model_en\n",
    "    elif lang == 'ar':\n",
    "        tokenizer, model = tokenizer_ar, model_ar\n",
    "    else:\n",
    "        return text  # ignorer les langues non g√©r√©es\n",
    "\n",
    "    inputs = tokenizer([text], return_tensors=\"pt\", truncation=True)  \n",
    "    translated = model.generate(**inputs)\n",
    "    return tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "\n",
    "# AJOUT : Appliquer la traduction sur question_clean si ce n‚Äôest pas en fran√ßais\n",
    "df['question_clean'] = df['question_clean'].astype(str)\n",
    "df['question_clean'] = df['question_clean'].apply(translate_to_french)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "25e71d0a-57a4-44d6-bb75-84220c873c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0db355a-3ffb-4a36-b6ad-541e2c2d7817",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_age(text):\n",
    "    if not isinstance(text, str):\n",
    "        return -1\n",
    "\n",
    "    # Mapping pour termes non num√©riques\n",
    "    mapping = {\n",
    "        'newborn': 0.1,\n",
    "        'infant': 0.5,\n",
    "        'toddler': 2,\n",
    "        'child': 7,\n",
    "        'adolescent': 15,\n",
    "        'teenager': 15,\n",
    "    }\n",
    "\n",
    "    # D√©cades (\"thirties\", ...)\n",
    "    decades = {\n",
    "        \"twenties\": 25,\n",
    "        \"thirties\": 35,\n",
    "        \"forties\": 45,\n",
    "        \"fifties\": 55,\n",
    "        \"sixties\": 65,\n",
    "        \"seventies\": 75,\n",
    "        \"eighties\": 85,\n",
    "        \"nineties\": 95,\n",
    "    }\n",
    "\n",
    "    # Patterns regex\n",
    "    patterns = [\n",
    "        r'\\bI\\s*am\\s*(\\d{1,3})\\b',                                 # I am 23\n",
    "        r\"\\bI'm\\s*(\\d{1,3})\\b\",                                    # I'm 37\n",
    "        r'\\bI\\s*am\\s*(\\d{1,3})\\s*yrs?\\b',                          # I am 44yrs\n",
    "        r\"\\bI'm\\s*(\\d{1,3})\\s*yrs?\\b\",                             # I'm 44yrs\n",
    "        r'\\b(\\d{1,3})\\s*(years? old|yrs? old|yr old|yo|ans?)\\b',   # 37 years old, 23 yr old, 4 yo\n",
    "        r'\\b(\\d{1,3})\\s*-\\s*year[- ]*old\\b',                       # 6-year-old\n",
    "        r'\\b(\\d{1,3})\\s?(?:M|F|m|f)[, ]',                          # 43F, 56M,\n",
    "        r'\\b(age|√¢ge|Age|√Çge)\\s*:?[\\s]*?(\\d{1,3})\\b',              # Age: 53 ; √¢ge 21\n",
    "        r'\\b(\\d{1,3})\\s*[.,]',                                     # 34, with symptoms -- attention faux positifs\n",
    "        r'\\bmy\\s+\\d{1,3}-year-old',                                # my 7-year-old\n",
    "        r'\\b(\\d{1,3})\\b(?=\\s*[-]*year[- ]*old)',                   # 6-year-old, alt\n",
    "        r'\\baged\\s*(\\d{1,3})\\b',                                   # aged 54\n",
    "        r'\\bwhen I was (\\d{1,3})\\s*yrs?\\b',                        # when I was 2yrs\n",
    "    ]\n",
    "    for pat in patterns:\n",
    "        match = re.search(pat, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            for group in match.groups():\n",
    "                if group and group.isdigit():\n",
    "                    return int(group)\n",
    "            if match.lastindex and match.lastindex >= 2 and match.group(2).isdigit():\n",
    "                return int(match.group(2))\n",
    "\n",
    "    # Age √©crit en toutes lettres (\"twelve year old\")\n",
    "    match = re.search(r'\\b([a-z]+)\\s+year[s]? old\\b', text.lower())\n",
    "    if match:\n",
    "        words2num = {\n",
    "            'zero': 0, 'one': 1, 'two':2, 'three':3, 'four':4, 'five':5, 'six':6, 'seven':7, 'eight':8,\n",
    "            'nine':9, 'ten':10, 'eleven':11, 'twelve':12, 'thirteen':13, 'fourteen':14, 'fifteen':15, 'sixteen':16,\n",
    "            'seventeen':17, 'eighteen':18, 'nineteen':19, 'twenty':20\n",
    "        }\n",
    "        val = match.group(1)\n",
    "        if val in words2num:\n",
    "            return words2num[val]\n",
    "\n",
    "    # Age au format mois (\"10 months old\", \"7 mo\", ...)\n",
    "    match = re.search(r'(\\d{1,2})\\s*(months?|mos?|mo\\.?)\\s*(old)?\\b', text, re.IGNORECASE)\n",
    "    if match:\n",
    "        mois = int(match.group(1))\n",
    "        return round(mois / 12, 2)  # √Çge en ann√©es avec 2 d√©cimales\n",
    "\n",
    "    # Age contextuel sur famille proche (\"my daughter is 17\", ...)\n",
    "    relatives = [\n",
    "        'daughter', 'son', 'wife',\n",
    "        'mother[- ]in[- ]law', 'father[- ]in[- ]law', 'husband',\n",
    "        'mother', 'father', 'sister', 'brother'\n",
    "    ]\n",
    "    rel_pat = r'\\bmy\\s+(?:' + '|'.join(relatives) + r')\\s*(?:is|aged)?\\s*([\\d]{1,3})\\b'\n",
    "    match = re.search(rel_pat, text, re.IGNORECASE)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "\n",
    "    # Cas \"in his/her/their twenties\" etc.\n",
    "    match = re.search(r'in\\s+(?:his|her|their)?\\s*(twenties|thirties|forties|fifties|sixties|seventies|eighties|nineties)\\b', text.lower())\n",
    "    if match:\n",
    "        return decades[match.group(1)]\n",
    "\n",
    "    # Formulation anglaise¬†: \"one and a half years\"\n",
    "    match = re.search(r'(\\w+) and a half years?', text.lower())\n",
    "    if match:\n",
    "        words = {'one': 1, 'two': 2, 'three': 3, 'four': 4, 'five': 5,\n",
    "                 'six': 6, 'seven': 7, 'eight': 8, 'nine': 9, 'ten': 10}\n",
    "        val = match.group(1)\n",
    "        if val in words:\n",
    "            return words[val] + 0.5\n",
    "\n",
    "    # Terme commun (\"infant\", \"toddler\", etc.)\n",
    "    for label, val in mapping.items():\n",
    "        if label in text.lower():\n",
    "            return val\n",
    "\n",
    "    return -1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_sex(text):\n",
    "    t = text.lower()\n",
    "    # \"male\" / \"female\" / \"man\" / \"woman\"\n",
    "    if re.search(r'\\bmale\\b|\\bman\\b|\\bm\\b|\\bboy\\b|\\bson\\b|\\bhusband\\b|\\bhe\\b|\\bhis\\b|\\bprostate\\b|\\bpenis\\b|\\btesticle\\b|\\bgentleman\\b|\\bdad\\b|\\bfather\\b|\\b(semen|sperm|testical|motility|penis|testicles)\\b', t, re.I):\n",
    "        return 'M'\n",
    "    elif re.search(r'\\bfemale\\b|\\bwoman\\b|\\bf\\b|\\bgirl\\b|\\bdaughter\\b|\\bwife\\b|\\bshe\\b|\\bher\\b|\\bpregnant\\b|\\bvagina\\b|\\buterus\\b|\\bovary\\b|\\bmenstruation\\b|\\bmother\\b|\\b(pregnan|embryo|IVF|fetal|delivery|uterus|ovary|menstruation|period|girlfriend|wife|partner is pregnant|married woman|dysmenorrhea)\\b', t, re.I):\n",
    "        return 'F'\n",
    "    # Indiquent un contexte masculin sans √™tre d√©cisif\n",
    "    if re.search(r'\\b(sexual dysfunction|semen analysis|sperm count|erectile dysfunction|testosterone|gynecomastia|foreskin|errection|scrotum|testicle|sack|my brother)\\b', t):\n",
    "        return 'M'\n",
    "    # Indiquent un contexte f√©minin sans √™tre d√©cisif\n",
    "    if re.search(r'\\b(fertility treatment|fetal|fetus|oocyte|embryo transfer|gestational|ins√©mination|dysmenorrhea|menstruation|vaginal|cervix|ovary|uterine|period|breast|menopause|ivf|iui|endometrio|cervix|pregnan|period|contraceptive|pregnancy|labia|pcos)|my sister\\b', t):\n",
    "        return 'F'\n",
    "    return 'U'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df['age'] = df['question'].apply(extract_age)\n",
    "df['sex'] = df['question'].apply(extract_sex)\n",
    "\n",
    "counts = df['sex'].value_counts()\n",
    "total = len(df)\n",
    "\n",
    "# 4. Afficher les comptes et le ratio pour F/M/U\n",
    "print(counts)\n",
    "print(\"\\nRATIO (%) :\")\n",
    "for cat in ['F', 'M', 'U']:\n",
    "    pct = 100 * counts.get(cat, 0) / total\n",
    "    print(f\"{cat}: {counts.get(cat, 0)} ({pct:.1f}%)\")\n",
    "\n",
    "\n",
    "# Nombre de lignes avec √¢ge d√©tect√© (diff√©rent de -1)\n",
    "nb_age_detected = (df['age'] != -1).sum()\n",
    "nb_total = len(df)\n",
    "taux_age = nb_age_detected / nb_total * 100\n",
    "\n",
    "print(f\"Taux de lignes avec √¢ge d√©tect√©¬†: {nb_age_detected} / {nb_total} = {taux_age:.1f}%\")\n",
    "#print(df[['question', 'age', 'sex']].sample(10))\n",
    "\n",
    "\n",
    "# 1. Appliquer le pr√©processing\n",
    "#df['question_clean'] = df['question'].progress_apply(preprocess) > d√©j√† appliquer sur le dataset charg√© car process long 20 min\n",
    "\n",
    "\n",
    "#TF IDF \n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.pipeline import Pipeline  # peut rester, mais on peut aussi utiliser sklearn.pipeline.Pipeline\n",
    "from textblob import TextBlob\n",
    "import numpy as np\n",
    "\n",
    "# 1. Encoder la colonne triage en 0/1\n",
    "df['label'] = df['triage'].map({'non-urgent': 0, 'urgent': 1})\n",
    "\n",
    "# 2. Pr√©paration des features\n",
    "X = df[['question_clean', 'age', 'sex']].copy()\n",
    "y = df['label']\n",
    "\n",
    "# 3. Nettoyage des colonnes\n",
    "X.loc[:, 'age'] = X['age'].replace(-1, np.nan)\n",
    "X.loc[:, 'age'] = X['age'].fillna(X['age'].median())\n",
    "X.loc[:, 'sex'] = X['sex'].fillna('U')  # U = Unknown\n",
    "\n",
    "# 4. Suppression des lignes sans texte vide\n",
    "mask_txt = X['question_clean'].notna() & (X['question_clean'].str.strip() != \"\")\n",
    "X = X.loc[mask_txt].copy()\n",
    "y = y.loc[X.index]\n",
    "\n",
    "# 5. Ajout de l‚Äôanalyse de sentiment\n",
    "X.loc[:, 'sentiment'] = X['question_clean'].apply(\n",
    "    lambda x: TextBlob(x).sentiment.polarity if isinstance(x, str) else 0\n",
    ")\n",
    "\n",
    "# 6. Split train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=0\n",
    ")\n",
    "\n",
    "# 7. Pr√©traitements\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('txt', TfidfVectorizer(ngram_range=(1, 2), max_features=1000), 'question_clean'),\n",
    "        ('age', StandardScaler(), ['age']),\n",
    "        ('sex', OneHotEncoder(drop='first', handle_unknown='ignore'), ['sex']),\n",
    "        ('sentiment', 'passthrough', ['sentiment'])\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 8. Pipeline sans SMOTE, avec class_weight='balanced'\n",
    "pipeline = Pipeline([\n",
    "    ('preproc', preprocessor),\n",
    "    ('clf', LogisticRegression(solver='liblinear', class_weight='balanced'))\n",
    "])\n",
    "\n",
    "# 9. Entra√Ænement\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# 10. √âvaluation\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# --- ROC CURVE ---\n",
    "y_proba = pipeline.predict_proba(X_test)[:,1]  # proba \"urgent\"\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=1, linestyle='--')\n",
    "plt.xlabel('Taux de faux positifs (1-Sp√©cificit√©)')\n",
    "plt.ylabel('Taux de vrais positifs (Sensibilit√©)')\n",
    "plt.title('Courbe ROC - Mod√®le de triage urgence')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# --- MATRICE DE CONFUSION ---\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Non-urgent\", \"Urgent\"])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Matrice de confusion')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "# Probabilit√© d'√™tre \"urgent\"\n",
    "y_proba = pipeline.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Calcul courbe PR\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
    "avg_prec = average_precision_score(y_test, y_proba)\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(recall, precision, color='purple', lw=2, label=f'Courbe PR (AP={avg_prec:.2f})')\n",
    "plt.xlabel('Recall (Sensibilit√© / Taux de vrais positifs)')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Courbe Precision-Recall - Mod√®le de triage urgence')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
